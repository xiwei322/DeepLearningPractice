{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S22_Tut_1_Artificial_Pigeon_Brain.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_7PSrzbdQbT"
      },
      "source": [
        "# Tutorial 1 - Artificial Pigeon Brain \n",
        "\n",
        "## Last Lecture\n",
        "* Biological Neural Networks (BNNs)\n",
        "* Artificial Neurons Networks (ANNs)\n",
        "* Forward Pass\n",
        "* Activation Functions\n",
        "* Mean Squared Error and Cross-Entropy\n",
        "* Gradient Descent and Backpropagation\n",
        "* Nonlinearity\n",
        "\n",
        "Sample code:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vk1HIgTbbKt"
      },
      "source": [
        "#### Example: 1-layer ANN with MSE and Gradient Descent\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tmvmSv7bSvm"
      },
      "source": [
        "import math\n",
        "\n",
        "# data (first column is the bias term)\n",
        "x = [[1, 0.1,-0.2], \n",
        "     [1,-0.1, 0.9], \n",
        "     [1, 1.2, 0.1], \n",
        "     [1, 1.1, 1.5]]\n",
        "\n",
        "# labels (desired output)\n",
        "t = [0, 0, 0, 1]\n",
        "\n",
        "# initial weights\n",
        "w = [1, -1, 1]\n",
        "\n",
        "iterations = 50\n",
        "learning = 10\n",
        "\n",
        "def simple_ann_MSE(x, w, t, iterations, learning):\n",
        "\n",
        "    E = []\n",
        "    \n",
        "    #iterate over epochs\n",
        "    for ii in range(iterations):\n",
        "        err = [] \n",
        "        y = []\n",
        "        #iterate over all the samples x\n",
        "        for n in range(len(x)):\n",
        "            v = 0\n",
        "            # compute w.x\n",
        "            for p in range(len(x[0])):\n",
        "                v = v + x[n][p]*w[p]\n",
        "            \n",
        "            #sigmoidal activation    \n",
        "            y.append(1 / (1 + math.e**(-v))) \n",
        "            \n",
        "            #MSE classification error\n",
        "            err.append((y[n]-t[n])**2)\n",
        "            \n",
        "            #gradient descent to compute new weights\n",
        "            for p in range(len(w)):\n",
        "                d = x[n][p]*(y[n]-t[n])*(1-y[n])*(y[n])\n",
        "                w[p] = w[p] - learning*d\n",
        "                \n",
        "        #sum up classification error\n",
        "        E.append(sum(err)/len(x))\n",
        "    \n",
        "    return (y, w, E)\n",
        "\n",
        "(y, w, E) = simple_ann_MSE(x, w, t, iterations, learning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TncfaRORb5tN"
      },
      "source": [
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv7VdSNxcFWT"
      },
      "source": [
        "#### Example: 1-layer ANN with Cross-Entropy and Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytnwNDitduho"
      },
      "source": [
        "def simple_ann_CE(x, w, t, iterations, learning):\n",
        "\n",
        "    E = []\n",
        "    \n",
        "    #iterate over epochs\n",
        "    for ii in range(iterations):\n",
        "        err = [] \n",
        "        y = []\n",
        "        \n",
        "        #iterate over all the samples x\n",
        "        for n in range(len(x)):\n",
        "            v = 0\n",
        "            \n",
        "            #compute w.x\n",
        "            for p in range(len(x[0])):\n",
        "                v = v + x[n][p]*w[p]\n",
        "            \n",
        "            #sigmoidal activation\n",
        "            y.append(1 / (1 + math.e**(-v))) \n",
        "            \n",
        "            #cross-entropy classification error\n",
        "            err.append(-t[n]*math.log(y[n]+ 0.000001) - (1-t[n])*math.log(1-y[n]+ 0.000001))\n",
        "\n",
        "            #gradient descent to compute new weights\n",
        "            for p in range(len(w)):\n",
        "                d = x[n][p]*(y[n]-t[n]) #cross_entropy\n",
        "                w[p] = w[p] - learning*d\n",
        "                \n",
        "        #sum up classification error\n",
        "        E.append(sum(err))\n",
        "    \n",
        "    return (y, w, E)\n",
        "\n",
        "(y, w, E) = simple_ann_CE(x, w, t, iterations, learning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnsiF6FDdvM7"
      },
      "source": [
        "print(y)\n",
        "print(E)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVrJJw84dbiJ"
      },
      "source": [
        "### Exploring the Iris dataset\n",
        "\n",
        "Let us modify the above code to work with the iris data set.\n",
        "\n",
        "To begin, load the iris data into Google Colab. Last time we had some difficulty with this, so it's suggested that you use Chrome or Chromium. Another approach to load the iris data is shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38Qu2LlAIDlQ"
      },
      "source": [
        "# use sklearn.datasets to load iris data\n",
        "from sklearn.datasets import load_iris\n",
        "features, labels = load_iris(return_X_y=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCcmOIFyIbge"
      },
      "source": [
        "The iris data has 150 samples spread across three classes: \n",
        "1. Iris-setosa, \n",
        "2. Iris-versicolor,\n",
        "3. Iris-virginica. \n",
        "\n",
        "There are three features used: \n",
        "1. sepal length in cm\n",
        "2. sepal width in cm\n",
        "3. petal length in cm\n",
        "4. petal width in cm\n",
        "\n",
        "To keep things simple, let us pick two of the classes and perform binary classification with our sample code. We will select **Iris-setosa** and **Iris-versicolor** to start:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features"
      ],
      "metadata": {
        "id": "Ved09UvOz9Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "id": "dC2-ymA3nTfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj2vXappwiUA"
      },
      "source": [
        "# classification iris-setosa and iris-versicolor\n",
        "import numpy as np\n",
        "\n",
        "indices = np.array(range(0,100))\n",
        "\n",
        "#setup x matrix\n",
        "x = np.zeros((len(indices), 4 + 1))\n",
        "x[:,1:5] = features[indices,:]\n",
        "\n",
        "#add bias column\n",
        "x[:,0] = np.ones(len(indices))\n",
        "\n",
        "#labels\n",
        "t = labels[indices]\n",
        "\n",
        "# initial weights\n",
        "w = np.random.rand(5)\n",
        "\n",
        "iterations = 100\n",
        "learning = 0.0001\n",
        "\n",
        "(y, w, E) = simple_ann_CE(x, w, t, iterations, learning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgPIaMRLzjXC"
      },
      "source": [
        "for i in range(len(t)):\n",
        "  print(t[i], y[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO8CMusz29hQ"
      },
      "source": [
        "We're able to successfully classify iris-setosa and iris-versicolor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKtAXhNZJUok"
      },
      "source": [
        "# classification iris-versicolor and iris virginica\n",
        "indices = np.array(range(50,150))\n",
        "\n",
        "#setup x matrix\n",
        "x = np.zeros((len(indices), 4 + 1))\n",
        "x[:,1:5] = features[indices,:]\n",
        "\n",
        "#add bias column\n",
        "x[:,0] = np.ones(len(indices))\n",
        "\n",
        "#labels\n",
        "t = labels[indices]-1\n",
        "\n",
        "# initial weights\n",
        "w = np.random.rand(5)\n",
        "\n",
        "iterations = 100\n",
        "learning = 0.0001\n",
        "\n",
        "\n",
        "(y, w, E) = simple_ann_CE(x, w, t, iterations, learning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5DlhozN2gOW"
      },
      "source": [
        "for i in range(len(t)):\n",
        "  print(t[i], y[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iwvTlW43L8h"
      },
      "source": [
        "The performance on the iris-versicolor and iris virginica is not as good. To find out why, we will try to visualize the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb_g2BEdU_Fu"
      },
      "source": [
        "### Visualize Iris Dataset\n",
        "Since the Iris dataset has only 4 inputs we can try to visualize it on a 2-dimensional plane to get a better idea of what is happening."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6SleB-DVS25"
      },
      "source": [
        "#scatter plot of iris-setosa and iris-versicolor\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "indices = np.array(range(0,100))\n",
        "\n",
        "selected_features = features[indices,:]\n",
        "selected_labels = labels[indices]\n",
        "\n",
        "feature_name = ['sepal length in cm', 'sepal width in cm', 'petal length in cm', 'petal width in cm']\n",
        "\n",
        "x_index = 0\n",
        "y_index = 1\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "plt.scatter(selected_features[:,x_index], selected_features[:,y_index], c= selected_labels)\n",
        "plt.colorbar(ticks=[0, 1, 2])\n",
        "plt.xlabel(feature_name[x_index])\n",
        "plt.ylabel(feature_name[y_index])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcLbUFswvBCE"
      },
      "source": [
        "# scatter plot of iris-versicolor and iris virginica\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "indices = np.array(range(50,150))\n",
        "\n",
        "selected_features = features[indices,:]\n",
        "selected_labels = labels[indices]\n",
        "\n",
        "feature_name = ['sepal length in cm', 'sepal width in cm', 'petal length in cm', 'petal width in cm']\n",
        "\n",
        "x_index = 0\n",
        "y_index = 1\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "plt.scatter(selected_features[:,x_index], selected_features[:,y_index], c= selected_labels)\n",
        "plt.colorbar(ticks=[0, 1, 2])\n",
        "plt.xlabel(feature_name[x_index])\n",
        "plt.ylabel(feature_name[y_index])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Cqsdyg1de8M"
      },
      "source": [
        "### Nonlinear Separation\n",
        "Our 1-layer ANN was only successful one of the binary classification combinations. A 1-layer ANN is unable to handle nonlinear separations (or decisions boundaries). To address this we can introduce a second layer known as a hidden layer. How could we do this?\n",
        "\n",
        "We can just include an additional 1-layer networks as shown in the image below. \n",
        "\n",
        "![alt text](https://miro.medium.com/max/1000/1*sX6T0Y4aa3ARh7IBS_sdqw.png)\n",
        "\n",
        "\n",
        "We would follow the same process as with the 1-layer network:\n",
        "\n",
        "1. write out the equations for the forward pass\n",
        "2. error term can stay the same MSE or Cross-Entropy\n",
        "3. Gradient descent would be applied now to two layers of weights\n",
        "\n",
        "First we would consider the forward pass for a 2-layer ANN. We could use a sigmoidal (logistic) activation function to keep things consistent with our earlier example. Note that the activation function will be applied once on the hidden layer, and also on the output layer.\n",
        "\n",
        "Computing the gradient with respect to the different layers of weights will become more difficult, but still manageable. There are just some additional terms in the chain rule. The second layer weights will be almost identical to what we computed for a 1-layer network, except the input will be the hidden layer activation.\n",
        "\n",
        "Instead of spending the time to compute gradients with each change to the network, which can be a fun mathematical exercise, we will instead focus on using the PyTorch libraries which handle all of this internally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g4umO_YStI4"
      },
      "source": [
        "### (Optional) Develop a 2-layer ANN\n",
        "Build a 2-layer network using cross-entropy. Determine the gradients with resepect to the layer 1 and layer 2 weights. How could you validate if the gradients were computed correctly?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXNbhFSFTJp1"
      },
      "source": [
        "#write code to build a 2-layer network using cross-entropy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiwXNn3SNZfu"
      },
      "source": [
        "### What is PyTorch?\n",
        "\n",
        "PyTorch is a scientific computing package that builds on the NumPy library to allow for the use of GPUs. It incorporates deep learning capabilities while maximizing flexibility and speed.\n",
        "\n",
        "### PyTorch Basics\n",
        "\n",
        "To use PyTorch you must first import the library\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TU7EUSNcVN9x"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bTAo6LzVJwg"
      },
      "source": [
        "#### Tensors\n",
        "Tensors are n-dimensional arrays that allow that can be used with a GPU to accelerate computing. There are several ways to work with Tensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MuUNQZ3UksM"
      },
      "source": [
        "# initialize a random tensor\n",
        "x = torch.rand(4, 3)\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmcpRB48VVFe"
      },
      "source": [
        "# initialize a tensor loaded with zeros\n",
        "x = torch.zeros(4, 3)\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI84uJM-VjyL"
      },
      "source": [
        "# initialized with data entered manually\n",
        "x = torch.tensor([2.1, 4.0, -5.2])\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "421on8nnWE3e"
      },
      "source": [
        "# initialized with data from numPy\n",
        "import numpy as np\n",
        "data = np.array([2.1, 4.0, -5.2])\n",
        "print(data)\n",
        "x = torch.tensor(data)\n",
        "print(x)\n",
        "\n",
        "# note you can easily convert fron tensor to numpy\n",
        "x_np = x.numpy()\n",
        "print(x_np)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_I8gINJWy1g"
      },
      "source": [
        "### Tensor size and shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUCzrV1nW5XA"
      },
      "source": [
        "# obtain size of tensor data structure\n",
        "x = torch.zeros(4, 3)\n",
        "print(x.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdPIVeeEXFO5"
      },
      "source": [
        "### Operations\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5AltppKXUZv"
      },
      "source": [
        "# tensor addition\n",
        "x = torch.rand(4, 3)\n",
        "y = torch.ones(4, 3)\n",
        "print(x + y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UsB6YlwX_p2"
      },
      "source": [
        "# tensor multiplication\n",
        "x = torch.rand(4, 3)\n",
        "y = torch.ones(4, 3)\n",
        "print(x * y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8W5LZlbXnMu"
      },
      "source": [
        "# Provide output tensor as argument\n",
        "result = torch.ones(4,3)\n",
        "torch.add(x, y, out = result)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxDlMlhnYWAN"
      },
      "source": [
        "# resize and reshape tensors\n",
        "x = torch.randn(4, 3)\n",
        "y = x.view(12)\n",
        "z = x.view(-1, 4)  # the size -1 is inferred from other dimensions\n",
        "print(x.size(), y.size(), z.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBcAFWNSZnPE"
      },
      "source": [
        "# convert one element tensor to a Python number\n",
        "x = torch.randn(1)\n",
        "print(x)\n",
        "print(x.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhBQXibrZy0Z"
      },
      "source": [
        "### Automatic Differentiation\n",
        "\n",
        "The PyTorch autograd package allows for easy computation of derivative. This is handled automatically using a define-by-run framework, which works as you write your code. \n",
        "\n",
        "To enable this feature you need to set the Tensor attribute .required_grat to True, at which point it begins to track all operations performed on it. After your computation have been completed you can call .backward() and all the gradients will be computed for you. The gradient for each tensor will be stored in the tensor attribute .grad.\n",
        "\n",
        "Each tensor also has a .grad_fn attribute which references Function that has created it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKqBTFK2cRZW"
      },
      "source": [
        "# Example computation of gradients\n",
        "x = torch.rand(4, 3, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "#perform some operations\n",
        "y = x + 10\n",
        "z = y*y\n",
        "out = z.mean()\n",
        "\n",
        "print(z, out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JDr3PgVdGMp"
      },
      "source": [
        "# compute the gradient with respect to the output\n",
        "out.backward()\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ntVfJhxUjtN"
      },
      "source": [
        "## Artificial Neural Networks in PyTorch\n",
        "In this example we will train an \"artificial pigeon\" to perform a digit recognition\n",
        "task. That is, we will use the MNIST dataset of hand-written digits, and train\n",
        "the pigeon to **recognize a small digit, namely a digit that is less than 3**.\n",
        "This problem is a **binary classification problem** we want to predict\n",
        "which of two classes an input image is a part of.\n",
        "\n",
        "### 1) Load MNIST Data\n",
        "The MNIST dataset contains hand-written digits that are 28x28=784 pixels large.\n",
        "Here are a few digits in the dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOu8rvDVMpSq"
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "# load the data\n",
        "mnist_train = datasets.MNIST('data', train=True, download=True)\n",
        "mnist_train = list(mnist_train)[:2000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3xCgqcwMvCl"
      },
      "source": [
        "# plot the first 18 images in the training data\n",
        "for k, (image, label) in enumerate(mnist_train[:18]):\n",
        "    plt.subplot(3, 6, k+1)\n",
        "    plt.imshow(image)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-DcwbkZ3Nlx"
      },
      "source": [
        "### 2) Defining the ANN Forward Pass\n",
        "Here is an implementation of the artificial pigeon brain in PyTorch.\n",
        "Don't worry if this code or the explanations don't make sense yet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-ueh5jh3I5p"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt # for plotting\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1) # set the random seed\n",
        "\n",
        "class Pigeon(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Pigeon, self).__init__()\n",
        "        self.layer1 = nn.Linear(784, 30) # 784 = 28x28 every pixel is a feature\n",
        "        \n",
        "        self.layer2 = nn.Linear(30, 1)\n",
        "    def forward(self, img):\n",
        "        flattened = img.view(-1, 28 * 28)\n",
        "        activation1 = self.layer1(flattened)\n",
        "        activation1 = F.relu(activation1)\n",
        "        activation2 = self.layer2(activation1)\n",
        "        return activation2\n",
        "\n",
        "pigeon = Pigeon()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi9lSSOtKhEE"
      },
      "source": [
        "In this network, there are 28x28 = 784 input neurons, to work with our 28x28 pixel images. We have a single output neuron and a hidden layer of 30 neurons.\n",
        "\n",
        "The variable `pigeon.layer1` contains information about the connectivity\n",
        "between the input layer and the hidden layer (stored as a matrix), and the\n",
        "biases (stored as a vector).\n",
        "\n",
        "Similarly, the variable `pigeon.layer2` contains information about the weights\n",
        "between the hidden layer and the output layer, and the bias.\n",
        "\n",
        "The weights and biases adjust during training, so they are called the model's\n",
        "**parameters**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82KQZkDlLx7j"
      },
      "source": [
        "# view parameters\n",
        "for w in pigeon.layer1.parameters():\n",
        "    print(w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86ubAvO03eMp"
      },
      "source": [
        "### 3) Test Network Forward Pass\n",
        "Here is an example of using the network to classify whether the\n",
        "image contains a small digit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtiBP83X247A"
      },
      "source": [
        "# make predictions for the first 10 images in mnist_train\n",
        "img_to_tensor = transforms.ToTensor() #transform the image data into a 28x28 matrix of numbers\n",
        "\n",
        "for k, (image, label) in enumerate(mnist_train[:10]):\n",
        "    inval = img_to_tensor(image)\n",
        "    outval = pigeon(inval)       # find the output activation given input\n",
        "    prob = torch.sigmoid(outval) # turn the activation into a probability\n",
        "    print(prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHkHYl0aO7ot"
      },
      "source": [
        "Since we haven't trained the network\n",
        "yet, the predicted probability of images containing a small digit \n",
        "is close to half. The \"pigeon\" is unsure.\n",
        "\n",
        "In order for the network to be useful, we need to actually train it, so\n",
        "that the weights are actually meaningful, non-random values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiUzssKS3ifv"
      },
      "source": [
        "### 4) Update Parameters using Gradient Descent with Cross-Entropy\n",
        "To update the parameters (weights) we will first use the network to make predictions, then compare the predictions\n",
        "agains the ground truth. To compare the predictions against actual values we'll compute a classification error using the Cross-Entropy equation.\n",
        "\n",
        "The classification error, that is how good or bad the prediction was compared to the actual values is more commonly referred to as the **loss** and Cross-Entropy is the **loss function**. The introduction of a loss function makes our problem a **optimization** problem: what set of parameters minimizes the loss across the training examples?\n",
        "\n",
        "Turning a learning problem into an optimization problem\n",
        "is actually a very subtle but important step in many machine learning tools,\n",
        "because it allows us to use tools from mathematical optimization.\n",
        "\n",
        "That there are **optimizers** that can tune the network parameters for\n",
        "us is also really, really cool. The gradient descent algorithm we derived in the last lecture is one example of an optimizer.\n",
        "\n",
        "For now, we will choose a standard loss function for a binary classification\n",
        "problem: the **binary cross-entropy loss**. We'll also choose\n",
        "a **stochastic gradient descent** optimizer. We'll talk about\n",
        "what these mean later in the course."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZhbr5Bw270k"
      },
      "source": [
        "# simplified training code to train `pigeon` on the \"small digit recognition\" task\n",
        "import torch.optim as optim\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.SGD(pigeon.parameters(), lr=0.005)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtHoiE6AYadE"
      },
      "source": [
        "Now, we can start to train the pigeon network, similar to the way we would train\n",
        "a real pigeon:\n",
        "\n",
        "1. We'll show the network pictures of digits, one by one\n",
        "2. We'll see what the network predicts\n",
        "3. We'll check the loss function for that example digit, comparing the network prediction against the ground truth\n",
        "4. We'll make a small update to the parameters to try and improve the loss for that digit\n",
        "5. We'll continue doing this many times -- let's say 1000 times\n",
        "\n",
        "For simplicity, we'll use 1000 images, and show the network each image only once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHmT89U0RM3C"
      },
      "source": [
        "for (image, label) in mnist_train[:1000]:\n",
        "    # actual ground truth: is the digit less than 3?\n",
        "    actual = torch.tensor(label < 3).reshape([1,1]).type(torch.FloatTensor)\n",
        "    # pigeon prediction\n",
        "    out = pigeon(img_to_tensor(image)) # step 1-2\n",
        "    # update the parameters based on the loss\n",
        "    loss = criterion(out, actual)      # step 3\n",
        "    loss.backward()                    # step 4 (compute the updates for each parameter)\n",
        "    optimizer.step()                   # step 4 (make the updates for each parameter)\n",
        "    optimizer.zero_grad()              # a clean up step for PyTorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfzAfZiuRQWx"
      },
      "source": [
        "It is very common to run into errors with changing different data types to tensors with the correct shape."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9Gieq973sDh"
      },
      "source": [
        "### 5) Test Updated Network Forward Pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-RgzimS3FEW"
      },
      "source": [
        "# make predictions for the first 10 images in mnist_train\n",
        "for k, (image, label) in enumerate(mnist_train[:10]):\n",
        "    print(label, torch.sigmoid(pigeon(img_to_tensor(image))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Otc8_sYjOk1I"
      },
      "source": [
        "Not bad! We'll use the probability 50% as the cutoff for making a \n",
        "discrete prediction. Then, we can compute the accuracy on the 1000\n",
        "images we used to train the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pluL_aN30CC"
      },
      "source": [
        "### 6)a) Validation of Network on Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-Qq5Bgh4Hx9"
      },
      "source": [
        "# computing the error and accuracy on the training set\n",
        "error = 0\n",
        "for (image, label) in mnist_train[:1000]:\n",
        "    prob = torch.sigmoid(pigeon(img_to_tensor(image)))\n",
        "    if (prob < 0.5 and label < 3) or (prob >= 0.5 and label >= 3):\n",
        "        error += 1\n",
        "print(\"Training Error Rate:\", error/1000)\n",
        "print(\"Training Accuracy:\", 1 - error/1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNozPD5XOk1L"
      },
      "source": [
        "The accuracy on those 1000 images is 96%, which is really good considering\n",
        "that we only showed the network each image only once.\n",
        "\n",
        "However, this accuracy is not representative of how well the network is doing,\n",
        "because the network was *trained* on the data. The network had a chance to\n",
        "see the actual answer, and learn from that answer. To get a better sense of\n",
        "the network's predictive accuracy, we should compute accuracy numbers on\n",
        "a **test set**: a set of images that were not seen in training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VplQogR24Aue"
      },
      "source": [
        "### 6)b) Validation of Network on Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blrK9sxS4IX5"
      },
      "source": [
        "# computing the error and accuracy on a test set\n",
        "error = 0\n",
        "for (image, label) in mnist_train[1000:2000]:\n",
        "    prob = torch.sigmoid(pigeon(img_to_tensor(image)))\n",
        "    if (prob < 0.5 and label < 3) or (prob >= 0.5 and label >= 3):\n",
        "        error += 1\n",
        "print(\"Test Error Rate:\", error/1000)\n",
        "print(\"Test Accuracy:\", 1 - error/1000)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}